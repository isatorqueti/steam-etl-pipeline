# üìâ Steam Data Pipeline 

Pipeline ETL *end-to-end* para ingest√£o, transforma√ß√£o e an√°lise hist√≥rica dos jogos mais populares da Steam usando **Python**, **Apache Airflow**, **Docker** e **PostgreSQL**.

---

## üìñ Vis√£o Geral

O objetivo deste projeto √© monitorar tend√™ncias de jogos em tempo real, criando um hist√≥rico ("s√©rie temporal") para analisar quais jogos est√£o ganhando ou perdendo popularidade.

- **Extra√ß√£o:** Coleta dados da API oficial da Steam.
- **Transforma√ß√£o:** Limpeza, normaliza√ß√£o e *timestamping* dos dados.
- **Carga:** Insere os dados processados em um banco PostgreSQL.
- **Orquestra√ß√£o:** Todo o fluxo √© automatizado via **Apache Airflow**.

---

## üèóÔ∏è Arquitetura do Pipeline

<img src='arquitetura_de_dados_draw.png' alt='Arquitetura do Pipeline ETL'>

---

## üõ†Ô∏è Stack Tecnol√≥gica

### Core
- **Python** - Linguagem principal
- **Apache Airflow** - Orquestra√ß√£o do pipeline
- **PostgreSQL** - Banco de dados relacional
- **Docker & Docker Compose** - Containeriza√ß√£o

### Bibliotecas Python
- **pandas** - Manipula√ß√£o e transforma√ß√£o de dados
- **requests** - Requisi√ß√µes HTTP para a API
- **SQLAlchemy** - ORM para intera√ß√£o com o banco de dados
- **psycopg2** - Driver PostgreSQL
- **python-dotenv** - Gerenciamento de vari√°veis de ambiente

### Outras Ferramentas
- **Redis** - Message broker para Celery
- **Jupyter Notebook** - An√°lise explorat√≥ria de dados
- **UV** - Gerenciador de pacotes Python r√°pido

---

## üóÇÔ∏è Estrutura do Projeto

```text
steam-etl-pipeline/
‚îú‚îÄ‚îÄ config/                # Configura√ß√µes do pipeline (ignorado pelo git)
‚îÇ   ‚îî‚îÄ‚îÄ .env               
‚îú‚îÄ‚îÄ dags/                  # Diret√≥rio mapeado para o Airflow
‚îÇ   ‚îú‚îÄ‚îÄ steam_dag.py       
‚îú‚îÄ‚îÄ data/                  # Dados extra√≠dos (ignorado pelo git)
‚îú‚îÄ‚îÄ logs/                  # Logs do Airflow (ignorado pelo git)
‚îú‚îÄ‚îÄ plugins/               # Plugins do Airflow (ignorado pelo git)
‚îú‚îÄ‚îÄ notebooks/             # Notebook de an√°lise e explora√ß√£o
‚îú‚îÄ‚îÄ src/                   # C√≥digo fonte do ETL
‚îÇ   ‚îú‚îÄ‚îÄ extract_data.py
‚îÇ   ‚îú‚îÄ‚îÄ transform_data.py
‚îÇ   ‚îî‚îÄ‚îÄ load_data.py
‚îú‚îÄ‚îÄ .env                   
‚îú‚îÄ‚îÄ docker-compose.yaml    # Orquestra√ß√£o dos containers
‚îú‚îÄ‚îÄ main.py                # Usado para testar o pipeline
‚îî‚îÄ‚îÄ README.md
```

## üß© Principais Arquivos

- `dags/steam_dag.py`: DAG principal do Airflow
- `src/extract_data.py`: Fun√ß√£o de extra√ß√£o da API
- `src/transform_data.py`: Fun√ß√µes de transforma√ß√£o
- `src/load_data.py`: Fun√ß√£o de carga no PostgreSQL
- `main.py`: Execu√ß√£o manual do pipeline

---

## üîç Detalhamento das Etapas

### üì• **ETAPA 1: EXTRACT**

**Arquivo:** [`src/extract_data.py`](src/extract_data.py)

**O que faz:**
1. Realiza requisi√ß√µes GET para os endpoints da Steam Web API:
- ISteamChartsService/GetGamesByConcurrentPlayers (Ranking)
- IStoreService/GetAppList (Nomes dos Jogos)
2. Salva os dados brutos em formato JSON na pasta dags/data/.

**Dados coletados:**
- Rank atual
- ID do Jogo (AppID)
- N√∫mero de jogadores simult√¢neos
- Pico de jogadores nas √∫ltimas 24h

---

### üîÑ **ETAPA 2: TRANSFORM**

**Arquivo:** [`src/transform_data.py`](src/transform_data.py)

**O que faz:**

#### 2.1 **Cria√ß√£o dos DataFrames**
- L√™ os arquivos JSON
- Converte para DataFrame Pandas

#### 2.2 **Merge e Limpeza**
- Cruza as informa√ß√µes do Ranking com a lista de Apps (merge via appid).
- Remove colunas desnecess√°rias para a an√°lise (ex: last_modified, price_change_number).
- Renomeia as colunas para padroniza√ß√£o de nomes claros.

#### 2.3 **Tratamento de dados Nulos**
- Para 'game_title' null, utilizamos "Desconhecido".

#### 2.4 **Cria√ß√£o de S√©rie Temporal**
- Adiciona a coluna 'extracted_at' com data/hora atual.

**Resultado:** DataFrame limpo, estruturado e pronto para an√°lise

---

### üíæ **ETAPA 3: LOAD**

**Arquivo:** [`src/load_data.py`](src/load_data.py)

**O que faz:**

#### 3.1 **Conex√£o com o banco de dados**
```python
engine = create_engine(
    f"postgresql+psycopg2://{user}:{password}@{host}:5432/{database}"
)
```

#### 3.2 **Inser√ß√£o dos dados**
```python
df.to_sql(
        name=table_name,
        con=engine,
        if_exists='append',
        index=False
    )
```

#### 3.3 **Valida√ß√£o**
- Faz um `SELECT COUNT(*)` para verificar total de registros
- Loga o resultado para auditoria


---

## üê≥ Infraestrutura e Orquestra√ß√£o

### 1. Docker (Containeriza√ß√£o)
O projeto √© totalmente isolado utilizando containers Docker, garantindo que o pipeline funcione em qualquer m√°quina sem conflitos de depend√™ncia.
- **Servi√ßos:** O `docker-compose.yaml` orquestra m√∫ltiplos servi√ßos simultaneamente: Airflow (Webserver, Scheduler, Triggerer) e o banco de dados PostgreSQL.
- **Persist√™ncia de Dados:** Utilizamos volumes Docker para garantir que os dados do banco (`postgres-db-volume`) n√£o sejam perdidos ao reiniciar os containers.
- **Networking:** Configura√ß√£o de rede interna para permitir a comunica√ß√£o direta entre o Airflow e o Postgres.

### 2. Apache Airflow (Orquestra√ß√£o)
O gerenciamento do fluxo de dados √© feito pelo Airflow.
- **TaskFlow API:** O c√≥digo utiliza a abordagem moderna do Airflow 2.0+ (`@dag`, `@task`), tornando o c√≥digo mais limpo e leg√≠vel.
- **Depend√™ncias:** O fluxo √© linear (`extract` >> `transform` >> `load`), garantindo que uma etapa s√≥ inicie se a anterior for conclu√≠da com sucesso.
- **Idempot√™ncia:** O pipeline foi desenhado para rodar m√∫ltiplas vezes sem quebrar, apenas adicionando novos registros hist√≥ricos.

---

## ‚öôÔ∏è Como Executar

### 1. Pr√©-requisitos
- **Docker** e **Docker Compose** instalados na m√°quina.
- Uma **API Key da Steam** (voc√™ pode obter gratuitamente [aqui](https://steamcommunity.com/dev/apikey)).

### 2. Configura√ß√£o do Ambiente

Crie um arquivo chamado `.env` dentro da pasta `config/` e preencha com suas credenciais:

```env
# Configura√ß√µes da API Steam
API_KEY=sua_chave_aqui

# Configura√ß√µes do Banco de Dados (Postgres)
# Nota: 'host.docker.internal' permite que o container acesse o host/outros servi√ßos
DB_HOST=host.docker.internal
DB_NAME=steam_data
DB_USER=airflow
DB_PASS=airflow
DB_PORT=5432
```
### 3. Inicializa√ß√£o

Na raiz do projeto, abra o terminal e execute:

```bash
# Sobe os containers em segundo plano (Detached mode)
docker-compose up -d
```

### 4. Acessando o Projeto
1.  Aguarde alguns instantes para a inicializa√ß√£o completa dos servi√ßos.
2.  Acesse a interface do Airflow em: [http://localhost:8080](http://localhost:8080)
3.  Fa√ßa login com as credenciais padr√£o:
    - **Usu√°rio:** `airflow`
    - **Senha:** `airflow`
4.  Localize a DAG **`steam_pipeline`**, ative o bot√£o (Unpause/Toggle Azul) e dispare a execu√ß√£o (Trigger/Play ‚ñ∂Ô∏è).

---

## üìå Refer√™ncia

Este projeto foi inspirado em um conte√∫do do canal [**vbluuiza**](https://youtu.be/I8qPqbXQBDU?si=lbhwEALHXY7vN4NN) e adaptado para fins de aprendizado.

---

## üë©‚Äçüíª Desenvolvido por

- **Nome:** Isadora Torqueti
- **GitHub:** https://github.com/isatorqueti
- **Linkedin:** https://www.linkedin.com/in/isadoratorqueti/